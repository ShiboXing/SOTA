{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# from common_utils import DATA_HOME\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from collections import Counter\n",
    "\n",
    "# dataset = \"playground-series-s4e2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_HOME\n",
    "train_data = pd.read_csv(\n",
    "    \"/Users/anyxling/datasets/playground-series-s4e2/train.csv\", index_col=0\n",
    ")\n",
    "# print(train_data.describe())\n",
    "train_data.shape\n",
    "# set(train_data[\"CALC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\n",
    "    \"/Users/anyxling/datasets/playground-series-s4e2/test.csv\", index_col=0\n",
    ")\n",
    "# print(test_data.describe())\n",
    "test_data.shape\n",
    "# test_data[test_data[\"CALC\"]==\"Always\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([train_data, test_data])\n",
    "train_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers\n",
    "outliers = []\n",
    "for col in train_test.select_dtypes(include=\"float\").columns:\n",
    "    col_mean = train_test[col].mean()\n",
    "    col_std = train_test[col].std()\n",
    "    col_outliers = train_test[\n",
    "        (train_test[col] > col_mean + 3 * col_std)\n",
    "        | (train_test[col] < col_mean - 3 * col_std)\n",
    "    ]\n",
    "    for idx in col_outliers.index:\n",
    "        outliers.append((col, idx))\n",
    "\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle outliers\n",
    "for outlier in outliers:\n",
    "    col, idx = outlier\n",
    "    val = train_test[col][idx]\n",
    "    col_mean = train_test[col].mean()\n",
    "    col_std = train_test[col].std()\n",
    "    if val > col_mean + 3 * col_std:\n",
    "        train_test[col][idx] = col_mean + 3 * col_std\n",
    "    if val < col_mean - 3 * col_std:\n",
    "        train_test[col][idx] = col_mean - 3 * col_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "sc = StandardScaler()\n",
    "num_cols = train_test.select_dtypes(include=\"float\").columns\n",
    "train_test[num_cols] = sc.fit_transform(train_test[num_cols])\n",
    "train_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical features\n",
    "str_cols = train_test.iloc[:, :-1].select_dtypes(include=[\"object\"]).columns\n",
    "le_features = LabelEncoder()\n",
    "for col in str_cols:\n",
    "    train_test[col] = le_features.fit_transform(train_test[col])\n",
    "    print(col, dict(zip(le_features.classes_, range(len(le_features.classes_)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "test_encoded = train_test[pd.isna(train_test[\"NObeyesdad\"])]\n",
    "train_encoded = train_test[pd.notna(train_test[\"NObeyesdad\"])]\n",
    "\n",
    "# encode labels in train data\n",
    "le_labels = LabelEncoder()\n",
    "y_train_encoded = le_labels.fit_transform(train_encoded.iloc[:, -1])\n",
    "print(len(y_train_encoded))\n",
    "# train_encoded.iloc[:, -1] = le_labels.fit_transform(train_encoded.iloc[:, -1])\n",
    "# train_encoded.iloc[:, -1] = train_encoded.iloc[:, -1].astype(int)\n",
    "# print(train_encoded.iloc[:, -1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if train data is imbalanced or not\n",
    "label_dist = Counter(y_train_encoded)\n",
    "print(label_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    train_encoded.iloc[:, :-1], y_train_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.5, random_state=42\n",
    ")\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search to try out different combinations of parameters\n",
    "param_grid = {\n",
    "    \"C\": [1, 10, 100],\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\", 1, 0.1],\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score, average=\"macro\"),\n",
    "    \"recall\": make_scorer(recall_score, average=\"macro\"),\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "grid_search = GridSearchCV(\n",
    "    svm, param_grid, scoring=scoring, refit=\"accuracy\", verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best combination of parameters and its score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "val_accuracy = best_model.score(X_val, y_val)\n",
    "print(\"Validation accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the validation accuracy with the baseline model\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")  # naive classifier, majority vote\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "print(\"accuracy score for dummy classifier:\", accuracy_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the performance's good, apply on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(test_encoded.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_pred = le_labels.inverse_transform(y_pred)\n",
    "decoded_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_idx = pd.read_csv(\"/Users/anyxling/datasets/playground-series-s4e2/test.csv\")\n",
    "sub = pd.DataFrame({\"id\": test_data_idx[\"id\"], \"NObeyesdad\": decoded_pred})\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
