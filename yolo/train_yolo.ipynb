{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x247180510d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "from models import YOLOv1\n",
    "from data.VOC_Dataset import VOC_Dataset\n",
    "from common_utils import DATA_HOME\n",
    "\n",
    "from ipdb import set_trace\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import cpu_count\n",
    "import random \n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class dict:  {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "_voc_root = f\"{DATA_HOME}/VOCdevkit/VOC2007\"\n",
    "voc_ds = VOC_Dataset(_voc_root)\n",
    "train_indices, test_indices = train_test_split(list(range(len(voc_ds))), test_size=0.2, random_state=1)\n",
    "train_set = Subset(voc_ds, train_indices)\n",
    "test_set = Subset(voc_ds, test_indices)\n",
    "\n",
    "BS = 2 # batch size\n",
    "S = 7 # num of rows/cols\n",
    "C = 20 # num of classes\n",
    "B = 2 # num of bounding boxes\n",
    "BOX = 5 # num of values for each bbox\n",
    "\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 1\n",
    "\n",
    "def inf_test_gen(data_loader):\n",
    "    while True:\n",
    "        for id, batch in enumerate(data_loader):\n",
    "            yield id, batch\n",
    "        \n",
    "\n",
    "def collate_fn(data):\n",
    "    # output: 7 * 7 * 30\n",
    "    # S * S * ((x, y, w, h, confidence) * B=2 + C=20)\n",
    "    # make img batch and label batch\n",
    "    imgs, labels, classes = zip(*data)\n",
    "    imgs = torch.tensor(np.array(imgs), dtype=torch.float16)\n",
    "    stride = 1. / S\n",
    "    label_mat = torch.zeros((len(imgs), BOX * B + C, S, S), dtype=torch.float16)\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            x, y, _, _= labels[i][j]\n",
    "            xi, yi = int(x // stride), int(y // stride)\n",
    "            cell = label_mat[i, :, xi, yi]\n",
    "            cell[:5] = cell[5:10] = torch.tensor([*labels[i][j], 1], dtype=torch.float16) # assign coords\n",
    "            cell[BOX * B + classes[i][j]] = 1.0 # assign class\n",
    "    return imgs, label_mat\n",
    "    \n",
    "if platform.system() == \"Windows\":\n",
    "    train_loader = DataLoader(train_set, batch_size=BS, pin_memory=False, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, pin_memory=False, shuffle=True, collate_fn=collate_fn)\n",
    "    save_dir = os.path.expanduser(\"~/YOLO_MODELS\")\n",
    "else:\n",
    "    train_loader = DataLoader(train_set, batch_size=BS, pin_memory=False, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, pin_memory=False, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
    "    save_dir = os.path.expanduser(os.environ[\"YOLO_MODELS\"])\n",
    "test_loader = inf_test_gen(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLOv1().to(device=device).half()\n",
    "\n",
    "# Apply initialization to the model's weights\n",
    "# for name, param in yolo.named_parameters():\n",
    "#     if 'weight' in name:\n",
    "#         torch.nn.init.kaiming_normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import IOU\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "def yolo_loss(yhat: torch.Tensor, y: torch.Tensor, loss_df: pd.DataFrame):\n",
    "    \"\"\"calcalate batch yolo loss, @param res_mat: (batch_size, B*5+C, S, S)\"\"\"\n",
    "    loss1 = loss2 = loss3 = loss4 = loss5 = 0.\n",
    "    # calculate loss for every bounding box in every cell\n",
    "    loss1 = (y[:, 4, :, :] * (lambda_coord * mse_loss(yhat[:, :2, :, :], y[:, :2, :, :]) + mse_loss(yhat[:, 5:7, :, :], y[:, 5:7, :, :]))).sum()\n",
    "    loss2 = (y[:, 4, :, :] * (lambda_coord * (yhat[:, 2:4, :, :].sqrt() - y[:, 2:4, :, :].sqrt()) ** 2 + (yhat[:, 7:9, :, :].sqrt() - y[:, 7:9, :, :].sqrt()) ** 2)).sum()\n",
    "    loss3 = (y[:, 4, :, :] * mse_loss(yhat[:, 4, :, :], y[:, 4, :, :])).sum()\n",
    "    loss4 = lambda_noobj * (yhat[:, 4, :, :][y[:, 4, :, :]==0] ** 2 + yhat[:, 9, :, :][y[:, 9, :, :]==0] ** 2).sum()\n",
    "    obj_mask = (y[:, 4, :, :] == 1).unsqueeze(1).expand(-1, 20, -1, -1)\n",
    "    loss5 = mse_loss(y[:, -C:, :, :][obj_mask], yhat[:, -C:, :, :][obj_mask]).sum()\n",
    "\n",
    "    loss_df = pd.concat((loss_df, pd.DataFrame([[loss1.item(), loss2.item(), loss3.item(), loss4.item(), loss5.item()]], columns=loss_df.columns)), ignore_index=True)\n",
    "    return loss1 + loss2 + loss3 + loss4 + loss5, loss_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save_dir: C:\\Users\\sxing/YOLO_MODELS\n",
      "train_set size: 4008, val_set size: 1003\n",
      "Epoch 1, iteration: 1000, train loss: 20.8125, val loss: 12.3359375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m \u001b[39m# forward-propagate\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m res \u001b[39m=\u001b[39m yolo(img\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice))\n\u001b[0;32m     34\u001b[0m \u001b[39m# print(res.shape, len(labels[2]), classes[2])\u001b[39;00m\n\u001b[0;32m     35\u001b[0m train_loss, train_loss_df \u001b[39m=\u001b[39m yolo_loss(res, label\u001b[39m.\u001b[39mto(device), train_loss_df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sxing\\SOTA\\yolo\\models\\yolo.py:114\u001b[0m, in \u001b[0;36mYOLOv1.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    109\u001b[0m h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures(img)\n\u001b[0;32m    110\u001b[0m \u001b[39m# h1 = self.conv1(img)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# h2 = self.conv2(h1)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m# h3 = self.conv3(h2)\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m# h4 = self.conv4(h3)\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m h5 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv5(h0)\n\u001b[0;32m    115\u001b[0m h6 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv6(h5)\n\u001b[0;32m    116\u001b[0m h7 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(h6\u001b[39m.\u001b[39mreshape(h6\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sys import modules\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# IO\n",
    "train_loss_fname = \"train_loss_latest.csv\"\n",
    "val_loss_fname = \"val_loss_latest.csv\"\n",
    "train_loss_df = pd.DataFrame(columns=[\"l1\", \"l2\", \"l3\", \"l4\", \"l5\"])\n",
    "val_loss_df = pd.DataFrame(columns=[\"l1\", \"l2\", \"l3\", \"l4\", \"l5\"])\n",
    "print(f\"model save_dir: {save_dir}\")\n",
    "print(f\"train_set size: {len(train_set)}, val_set size: {len(test_set)}\")\n",
    "files = [f for f in os.listdir(save_dir) if \".pth\" in f]\n",
    "epoch = 0\n",
    "\n",
    "if files:\n",
    "    max_f = max(files)\n",
    "    epoch = int(max_f.split('_')[1])\n",
    "    checkpoint_pth = save_dir+\"/\"+max_f\n",
    "    yolo.load_state_dict(torch.load(checkpoint_pth)[\"model\"])\n",
    "    print(\"loaded model checkpoint: \", checkpoint_pth)\n",
    "    train_loss_df = pd.read_csv(f\"{save_dir}/{train_loss_fname}\", index_col=None)\n",
    "    val_loss_df = pd.read_csv(f\"{save_dir}/{val_loss_fname}\", index_col=None)\n",
    "    print(\"loaded model loss dataframe\")\n",
    "\n",
    "for param in yolo.features.parameters():\n",
    "    param.requires_grad = False\n",
    "optimizer = optim.SGD(yolo.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "torch.cuda.empty_cache()\n",
    "# calculate training time\n",
    "start_time = time.time()\n",
    "while epoch < 100:\n",
    "    for _id, (img, label) in enumerate(train_loader):\n",
    "        if len(img) != len(label): continue # skip incomplete batch at last\n",
    "        # clear gradient graph\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward-propagate\n",
    "        res = yolo(img.to(device=device))\n",
    "        # print(res.shape, len(labels[2]), classes[2])\n",
    "        train_loss, train_loss_df = yolo_loss(res, label.to(device), train_loss_df)\n",
    "        # set_trace\n",
    "\n",
    "        # back-propagate\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate validation errors\n",
    "        _, (t_img, t_label) = next(test_loader)\n",
    "        with torch.no_grad():\n",
    "            res = yolo(t_img.to(device=device))\n",
    "            val_loss, val_loss_df = yolo_loss(res, t_label.to(device), val_loss_df)\n",
    "            \n",
    "        # perform IO\n",
    "        if epoch and epoch % 2 and _id == 1000:\n",
    "            # save model \n",
    "            checkpoint = {\n",
    "                'model': yolo.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'iteration': _id,\n",
    "                'train_loss': train_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_dir}/yolov1_{epoch}_{_id}.pth\")\n",
    "            \n",
    "            # show time elapse\n",
    "            print(f\"Epoch {epoch}, iteration: {_id}, train loss: {train_loss}, val loss: {val_loss}\")\n",
    "            train_loss_df.to_csv(f\"{save_dir}/{train_loss_fname}\")\n",
    "            val_loss_df.to_csv(f\"{save_dir}/{val_loss_fname}\")\n",
    "    epoch += 1\n",
    "    \n",
    "        # if _id == 2000:\n",
    "            # set_trace()\n",
    "            # end_time = time.time()\n",
    "            # print(\"1000 iterations cost: {:.2f} seconds\".format(end_time - start_time))\n",
    "            # start_time = end_time\n",
    "        # showing the image with labels\n",
    "        # res_img = Image.fromarray((batch[0] * 255).permute(1, 2, 0).byte().numpy())\n",
    "        # draw = ImageDraw.Draw(res_img)\n",
    "        # for pc in labels[0]:\n",
    "            # draw.rectangle((448*pc[0], 448*pc[1], 448*pc[2], 448*pc[3]), outline=\"red\")\n",
    "        # res_img.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
